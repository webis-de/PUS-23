{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"arno_experiments.ipynb","provenance":[{"file_id":"1IK-QF90VpGAZmfH0XqDrD8doJfuC6B92","timestamp":1610636648085}],"collapsed_sections":["BG3rjcwdsAvH","kr9rzPh5zNIA","NMUyzRdnYUaP","6QliI0Pt7eQ3","p2wVTNRY7hB_","PgVJr7mPzwj_","DP2I4yzW0vl2","4k2Me_0RyhFO"],"authorship_tag":"ABX9TyP+ZjElG04SRJj/wZ0Jbqka"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n4eNz7RIrzow"},"source":["## Set globals"]},{"cell_type":"code","metadata":{"id":"PJwyfS7eiPWw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610731846864,"user_tz":-60,"elapsed":6765,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"c062ab9f-104c-4061-9a2f-fba97c0113db"},"source":["# set paths\n","path_extractions = '/content/drive/My Drive/WIKING/extractions'\n","path_git = '/content/drive/My Drive/WIKING/WIKING_git'\n","path_my_tools = '/content/drive/My Drive/my_tools'\n","\n","# infiles\n","file_CRISPR_de = path_extractions + '/wikipedia_entries/CRISPR_de' # get latest file version here: https://files.webis.de/wikipedia-tracing-innovations/\n","file_CRISPR_en = path_extractions + '/wikipedia_entries/CRISPR_en' # get latest file version here: https://files.webis.de/wikipedia-tracing-innovations/\n","file_CRISPR_gene_editing_en = path_extractions + '/wikipedia_entries/CRISPR_gene_editing_en' # get latest file version here: https://files.webis.de/wikipedia-tracing-innovations/ \n","file_events = path_git + '/data/CRISPR_events - events.csv'\n","file_accounts = path_git + '/data/CRISPR_events - accounts.csv'\n","\n","# mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# add git folder to path variable\n","import sys\n","sys.path.append(path_git + '/code')\n","\n","# import Wolfgang's classes:\n","from article.article import Article\n","\n","# import other modules\n","!pip install fuzzywuzzy # https://github.com/seatgeek/fuzzywuzzy\n","from fuzzywuzzy import fuzz\n","from fuzzywuzzy import process\n","import networkx as nx\n","!pip install python-louvain\n","from community import community_louvain\n","import re\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import itertools\n","import pprint\n","pp = pprint.PrettyPrinter(indent=4)\n","from lxml import etree\n","\n","# set working directory\n","os.chdir(path_git)\n","os.getcwd()\n","\n","# Load data\n","CRISPR_en = Article(file_CRISPR_en)\n","CRISPR_gene_editing_en = Article(file_CRISPR_gene_editing_en)\n","# events = pd.read_csv(file_events)\n","# accounts = pd.read_csv(file_accounts)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.18.0)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n","  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"],"name":"stderr"},{"output_type":"stream","text":["Requirement already satisfied: python-louvain in /usr/local/lib/python3.6/dist-packages (0.15)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from python-louvain) (2.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from python-louvain) (1.19.5)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->python-louvain) (4.4.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BG3rjcwdsAvH"},"source":["## Generic functions"]},{"cell_type":"code","metadata":{"id":"VA1WhsvGiqjB","executionInfo":{"status":"ok","timestamp":1610719705958,"user_tz":-60,"elapsed":65688,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}}},"source":["def find_all(text, keyphrase): # https://stackoverflow.com/questions/4664850/how-to-find-all-occurrences-of-a-substring\n","  ''' Returns indices for all matches of keyphrase '''\n","  start = 0\n","  while True:\n","    start = text.find(keyphrase, start)\n","    if start == -1: \n","      return\n","    yield start\n","    start += len(keyphrase) # use start += 1 to find overlapping matches"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kr9rzPh5zNIA"},"source":["## Experiments"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVPtbtHHA862","executionInfo":{"status":"ok","timestamp":1610731848508,"user_tz":-60,"elapsed":1636,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"b5309e2e-6fe3-419f-aea3-3ccaa4c89512"},"source":["keyphrases = [\n","    'Zhang',\n","    'Doudna',\n","    'history',\n","]\n","\n","revisions = [\n","    CRISPR_en.get_revision(revid=373434559),\n","    CRISPR_en.get_revision(revid=583141384),\n","    CRISPR_en.get_revision(revid=583141384),\n","]\n","\n","for keyphrase in keyphrases:\n","  print(keyphrase)\n","  for revision in revisions:\n","    print(revision.url)\n","    pp.pprint(revision.get_lr_contexts(keyphrase, lowerc=True))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Zhang\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=373434559\n","[]\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=583141384\n","[]\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=583141384\n","[]\n","Doudna\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=373434559\n","[]\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=583141384\n","[]\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=583141384\n","[]\n","history\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=373434559\n","[]\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=583141384\n","[('1', ''), ('', '')]\n","https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=583141384\n","[('1', ''), ('', '')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mli3ATBby6jI"},"source":["  # # dict comprehension (same as reference.get_identifiers())\n","  # nofollows = reference.source.xpath(\".//a[@rel='nofollow']\")\n","  # external_ids = {'DOI' if 'doi.org' in tag.get('href') \n","  #           else 'PMC' if 'pmc' in tag.get('href') \n","  #           else 'PMID':\n","  #                        tag.get('href').split('/')[-1] if 'doi.org' in tag.get('href')\n","  #                   else tag.get('href').split('/')[-1].split('PMC')[-1] if 'pmc' in tag.get('href')\n","  #                   else tag.get('href').split('/')[-1].split('?')[0] # = 'pubmed'\n","  #           for tag in nofollows \n","  #           if any(i in tag.get('href') for i in ['doi.org','pmc','pubmed'])}\n","  # print('\\t',external_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHa8hpV5DSDX","executionInfo":{"status":"ok","timestamp":1610667655859,"user_tz":-60,"elapsed":795,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"affd19af-6c6b-4fbe-ea98-a8ba75ab742d"},"source":["# revision = CRISPR_gene_editing_en.get_revision(revid=883727741)\n","# citation = {'PMID':'20056882', 'DOI':'10.1126/Science.1179555'}\n","# citations = [citation]\n","\n","# revisions = [revision]\n","# for revision in revisions:\n","#   for cit_indx, citation in enumerate(citations):\n","#     target_matches = set()\n","#     for id_type, id_num in citation.items():\n","#       target_matches.update([reference for reference in revision.get_references() if reference.get_identifiers()[id_type] == id_num])\n","#     if target_matches:\n","#       print()\n","#       print(revision.url)\n","#       for target_match in target_matches:\n","#         print()\n","#         print(target_match.get_number(), target_match.get_text())\n","#         print('#'*80,'\\n')\n","#         sections = revision.get_sections() #get sections\n","#         for section in list(target_match.linked_sections(sections)): #find sections reference links to\n","#           pp.pprint(section.get_text())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","https://en.wikipedia.org/w/index.php?title=CRISPR gene editing&oldid=883727741\n","\n","1 Horvath, Philippe; Barrangou, Rodolphe (2010-01-08). \"CRISPR/Cas, the immune system of bacteria and archaea\". Science (New York, N.Y.). 327 (5962): 167–170. doi:10.1126/science.1179555. ISSN 1095-9203. PMID 20056882.\n","################################################################################ \n","\n","'Diagram of the CRISPR prokaryotic antiviral defense mechanism. [1]'\n","\n","17 Horvath P, Barrangou R (January 2010). \"CRISPR/Cas, the immune system of bacteria and archaea\". Science. 327 (5962): 167–70. Bibcode:2010Sci...327..167H. doi:10.1126/science.1179555. PMID 20056882.\n","################################################################################ \n","\n","('CRISPR/Cas9 offers a high degree of fidelity and relatively simple '\n"," 'construction. It depends on two factors for its specificity: the target '\n"," 'sequence and the PAM. The target sequence is 20 bases long as part of each '\n"," 'CRISPR locus in the crRNA array. [15] A typical crRNA array has multiple '\n"," 'unique target sequences. Cas9 proteins select the correct location on the '\n"," \"host's genome by utilizing the sequence to bond with base pairs on the host \"\n"," 'DNA. The sequence is not part of the Cas9 protein and as a result is '\n"," 'customizable and can be independently synthesized . [17] [18] \\n')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_l9TB3a-Uu8o"},"source":["## Citation context"]},{"cell_type":"markdown","metadata":{"id":"FuoTk2KnYSm1"},"source":["### Define function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08ScrpYP5aLr","executionInfo":{"status":"ok","timestamp":1610730930688,"user_tz":-60,"elapsed":930,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"ff288276-0438-4ccd-fde4-88028c04cb48"},"source":["def get_lr_context(text, keyphrase, width=50, return_keyphrase=False):\n","  '''\n","  Generic function to get left and right context\n","  '''\n","  def find_all2(text, keyphrase): # https://stackoverflow.com/questions/4664850/how-to-find-all-occurrences-of-a-substring\n","    ''' Returns indices for all matches of keyphrase '''\n","    start = 0\n","    while True:\n","      start = text.find(keyphrase, start)\n","      if start == -1: \n","        return\n","      yield start\n","      start += len(keyphrase) # use start += 1 to find overlapping matches\n","  contexts = []\n","  for indx in find_all2(text, keyphrase):\n","    # context left\n","    left = ''\n","    for char in text[indx - width : indx][::-1]:\n","      if char == '\\n': # make '\\n' the boundary for context\n","        break\n","      left += char\n","    left = left[::-1].strip()\n","    # context right\n","    right = ''\n","    for char in text[indx + len(keyphrase): indx + len(keyphrase) + width]:\n","      if char == '\\n': # make '\\n' the boundary for context\n","        break\n","      right += char\n","    right = right.strip()\n","    # contexts.append((left, right))\n","    contexts.append((left, right) if not return_keyphrase else (left, keyphrase, right))\n","  return contexts\n","\n","get_lr_context('tsdf h hsdf hfjskkjhkg asdlkjkjlsef 9876669s87f hksjdfb baksjd 7666575 ad hkhgsf 87687666 a sef', '666', width=50, return_keyphrase=False)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('', '9s87f hksjdfb baksjd 7666575 ad hkhgsf 87687666 a'),\n"," ('jskkjhkg asdlkjkjlsef 9876669s87f hksjdfb baksjd 7',\n","  '575 ad hkhgsf 87687666 a sef'),\n"," ('9876669s87f hksjdfb baksjd 7666575 ad hkhgsf 87687', 'a sef')]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"oAa9nyB76RAJ"},"source":["def aggregate_lr_contexts(contexts)\n","\n","df = pd.DataFrame(contexts[cit_indx], columns=['revindx','timestamp','left_id','left','keyphrase','right','right_id','revid','url','user','userid','comment',])\n","    # determine communities of similarity for left and right contexts\n","    for side in ['left', 'right']:\n","      G = nx.Graph()\n","      unique_contexts = list(df[side].unique())\n","      length = len(unique_contexts)\n","      for query in unique_contexts:\n","        indx = unique_contexts.index(query) + 1\n","        if indx <= length:\n","          scores = process.extractBests(query, unique_contexts[unique_contexts.index(query) + 1:], score_cutoff=score_cutoff, limit=length, scorer=scorer) # https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/process.py\n","          edges = [(query, i[0]) for i in scores]\n","          G.add_edges_from(edges)\n","      # add community_ids for lonesome contexts as well\n","      unique_contexts = set(unique_contexts)\n","      community_dic = community_louvain.best_partition(G)\n","      if community_dic:\n","        community_contexts = set(community_dic.keys())\n","        lonesome_contexts = unique_contexts - community_contexts\n","        lonesome_dic = {k:v for v,k in enumerate(lonesome_contexts,start=max(community_dic.values())+1)}\n","        community_dic.update(lonesome_dic) # hiermit nun 'lid' und 'rid' Spalten befüllen...\n","      else:\n","        community_dic = {k:v for v,k in enumerate(unique_contexts)}\n","      # add community_ids to df\n","      column = df.apply(lambda row: community_dic[row[side]], axis=1)\n","      df['{}_id'.format(side)] = column\n","      df.sort_values(by=['left_id','timestamp'])\n","      if excel_path:\n","        citation_name = 'CITATION_{}'.format(citation['name'] if 'name' in citation else cit_indx)\n","        df.to_excel(excel_path + '/contexts_{}_{}.xlsx'.format(article.name, citation_name ))\n","      dfs[cit_indx] = df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFOqMXRH8LLn","executionInfo":{"status":"ok","timestamp":1610722834533,"user_tz":-60,"elapsed":918,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}}},"source":["def citations_in_context(citations, article, width=50, score_cutoff=75, scorer=fuzz.ratio, excel_path=None, allowed_identifiers=['DOI', 'PMC', 'PMID'],):\n","  ''' \n","  Returns dict of dfs for contexts to the left and to the right of each citation in list of citation\n","  citations: list of dictionaries of identifiers ('DOI', 'PMC', 'PMID'). Can also contain 'name' for file naming\n","  '''\n","  gen = article.yield_revisions()\n","  contexts = {k:[] for k in range(len(citations))}\n","  dfs = {k:None for k in range(len(citations))}\n","  for revision in gen:\n","    for cit_indx, citation in enumerate(citations):\n","      target_matches = set()\n","      identifiers = {k:v for k,v in citation.items() if v and any(k==i for i in allowed_identifiers)} # 'if v' is crucial or else  'if reference.get_identifiers()[id_type] == id_num' may later be true when matching the empty string\n","      for id_type, id_num in identifiers.items():\n","        target_matches.update([reference for reference in revision.get_references() if reference.get_identifiers()[id_type] == id_num])\n","      if target_matches:\n","        for target_match in target_matches: # occassionaly one source is double listed (with two different numbers) in Wikipedia reference lists\n","          keyphrase = '[{}]'.format(target_match.get_number()) # keyphrase = target_number!\n","          sections = revision.get_sections() #get sections\n","          for section in list(target_match.linked_sections(sections)): #find all sections to which the reference links\n","            text = section.get_text()\n","            for indx in find_all(text, keyphrase):\n","              # context left\n","              left = ''\n","              for char in text[indx - width : indx][::-1]:\n","                if char == '\\n': # make '\\n' the boundary for context\n","                  break\n","                left += char\n","              left = left[::-1].strip()\n","              # context right\n","              right = ''\n","              for char in text[indx + len(keyphrase): indx + len(keyphrase) + width]:\n","                if char == '\\n': # make '\\n' the boundary for context\n","                  break\n","                right += char\n","              right = right.strip()\n","              contexts[cit_indx].append((revision.index, revision.timestamp.string, 0, left, keyphrase, right, 0, revision.revid, revision.url, revision.user, revision.userid, revision.comment,))\n","  print('Analysing communities of similarity...')\n","  for cit_indx, citation in enumerate(citations):\n","    df = pd.DataFrame(contexts[cit_indx], columns=['revindx','timestamp','left_id','left','keyphrase','right','right_id','revid','url','user','userid','comment',])\n","    # determine communities of similarity for left and right contexts\n","    for side in ['left', 'right']:\n","      G = nx.Graph()\n","      unique_contexts = list(df[side].unique())\n","      length = len(unique_contexts)\n","      for query in unique_contexts:\n","        indx = unique_contexts.index(query) + 1\n","        if indx <= length:\n","          scores = process.extractBests(query, unique_contexts[unique_contexts.index(query) + 1:], score_cutoff=score_cutoff, limit=length, scorer=scorer) # https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/process.py\n","          edges = [(query, i[0]) for i in scores]\n","          G.add_edges_from(edges)\n","      # add community_ids for lonesome contexts as well\n","      unique_contexts = set(unique_contexts)\n","      community_dic = community_louvain.best_partition(G)\n","      if community_dic:\n","        community_contexts = set(community_dic.keys())\n","        lonesome_contexts = unique_contexts - community_contexts\n","        lonesome_dic = {k:v for v,k in enumerate(lonesome_contexts,start=max(community_dic.values())+1)}\n","        community_dic.update(lonesome_dic) # hiermit nun 'lid' und 'rid' Spalten befüllen...\n","      else:\n","        community_dic = {k:v for v,k in enumerate(unique_contexts)}\n","      # add community_ids to df\n","      column = df.apply(lambda row: community_dic[row[side]], axis=1)\n","      df['{}_id'.format(side)] = column\n","      df.sort_values(by=['left_id','timestamp'])\n","      if excel_path:\n","        citation_name = 'CITATION_{}'.format(citation['name'] if 'name' in citation else cit_indx)\n","        df.to_excel(excel_path + '/contexts_{}_{}.xlsx'.format(article.name, citation_name ))\n","      dfs[cit_indx] = df\n","  return dfs"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NMUyzRdnYUaP"},"source":["### Define citations"]},{"cell_type":"code","metadata":{"id":"QM8ey2ZbXz1N","executionInfo":{"status":"ok","timestamp":1610723234616,"user_tz":-60,"elapsed":1315,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}}},"source":["citations = [\n","        {'DOI':'10.1016/j.cell.2014.05.010',    'PMID':'24906146',  'PMC':'',         'name':'Hsu_et_al_2014'},\n","        {'DOI':'10.1126/science.1258096',       'PMID':'25430774',  'PMC':'',         'name':'Doudna_Charpentier_2014'},\n","        {'DOI':'10.1038/s41467-018-04252-2',    'PMID':'29765029',  'PMC':'5953931',  'name':'Adli_2018'},\n","        {'DOI':'10.1038/522020a',               'PMID':'26040877',  'PMC':'',         'name':'Ledford_2015'},\n","        {'DOI':'10.1126/science.341.6148.833',  'PMID':'23970676',  'PMC':'',         'name':'Pennisi_2013'},\n","        {'DOI':'10.1016/j.cell.2015.12.041',    'PMID':'26771483',  'PMC':'',         'name':'Lander_2016'},\n","        {'DOI':'10.1016/j.tim.2016.06.005',     'PMID':'27401123',  'PMC':'',         'name':'Mojica_Montoliu_2016'},\n","        {'DOI':'10.1111/febs.13766',            'PMID':'27234458',  'PMC':'',         'name':'Mojica_Rodriguez-Valera_2016'},\n","        {'DOI':'10.1128/JB.00580-17',           'PMID':'29358495',  'PMC':'5847661',  'name':'Ishino_et_al_2018'},\n","        {'DOI':'10.1016/j.coviro.2015.03.011',  'PMID':'25914022',  'PMC':'',         'name':'van_Erp_et_al_2015'},\n","        {'DOI':'10.1007/s12038-015-9532-6',     'PMID':'25740136',  'PMC':'',         'name':'Morange_2015a'},\n","        {'DOI':'10.1038/nbt.3160',              'PMID':'25748913',  'PMC':'',         'name':'Sherkov_2015'},\n","        {'DOI':'10.1007/978-3-642-34657-6_1',   'PMID':'',          'PMC':'',         'name':'Mojica_Garett_2013'},\n","        {'DOI':'10.1038/535342a',               'PMID':'27443723',  'PMC':'',         'name':'Ledford_2016a'},\n","        {'DOI':'10.1016/bs.pmbts.2017.10.001',  'PMID':'29150001',  'PMC':'',         'name':'Han_She_2017'},\n","        {'DOI':'10.1007/s12038-015-9575-8',     'PMID':'26648028',  'PMC':'',         'name':'Morange_2015b'},\n","        {'DOI':'10.1002/jgm.2963',              'PMID':'28623876',  'PMC':'',         'name':'Liang_et_al_2017'},\n","        {'DOI':'10.1007/s00284-018-1547-4',     'PMID':'30078067',  'PMC':'',         'name':'Javed_et_al_2018'},\n","        {'DOI':'10.1038/537460a',               'PMID':'27652544',  'PMC':'',         'name':'Ledford_2016b'},\n","        {'DOI':'10.1007/978-3-642-34657-6_3',   'PMID':'',          'PMC':'',         'name':'Makarova_Koonin_2013'},\n","        {'DOI':'10.1007/978-3-642-34657-6_11',  'PMID':'',          'PMC':'',         'name':'Horvath_et_al_2013'},\n","        {'DOI':'10.1038/d41586-020-02765-9',    'PMID':'33028993',  'PMC':'',         'name':'Ledford_Callaway_2020'}, \n","\n","]"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NsnB_qV-YY3r"},"source":["### Iterate over articles and apply function (main loop)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZD_9dV2YX9g","executionInfo":{"status":"ok","timestamp":1610727887219,"user_tz":-60,"elapsed":302237,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"6ec06319-7ba9-4026-82fc-fb4e9ee210aa"},"source":["for article in [\n","                CRISPR_en,\n","                # CRISPR_gene_editing_en,\n","                ]:\n","  print(article.name)\n","  citations_in_context(citations, article, width=100, score_cutoff=75, excel_path=path_extractions+'/citation_contexts')"],"execution_count":50,"outputs":[{"output_type":"stream","text":["CRISPR_en\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '.']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n","WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"kGgAMOM6zulH"},"source":["## Keyphrase contexts"]},{"cell_type":"markdown","metadata":{"id":"Gy6e--_37Z-F"},"source":["### Define function"]},{"cell_type":"code","metadata":{"id":"5z5OqAZm01QS"},"source":["def keyphrases_in_context(keyphrases, article, width=50, score_cutoff=75, scorer=fuzz.ratio, excel_path=None):\n","  ''' \n","  Returns list of dfs for contexts to the left and to the right of each keyphrase in list og keyphrases\n","  '''\n","  gen = article.yield_revisions()\n","  contexts = {k:[] for k in keyphrases}\n","  dfs = {k:None for k in keyphrases}\n","  for revision in gen:\n","    text = revision.get_text()\n","    text = text.lower()\n","    for keyphrase in keyphrases:\n","      if keyphrase in text:\n","        for indx in find_all(text, keyphrase):\n","          # context left\n","          left = ''\n","          for char in text[indx - width : indx][::-1]:\n","            if char == '\\n': # make '\\n' the boundary for context\n","              break\n","            left += char\n","          left = left[::-1].strip()\n","          # context right\n","          right = ''\n","          for char in text[indx + len(keyphrase): indx + len(keyphrase) + width]:\n","            if char == '\\n': # make '\\n' the boundary for context\n","              break\n","            right += char\n","          right = right.strip()\n","          contexts[keyphrase].append((revision.index, revision.timestamp.string, 0, left, keyphrase, right, 0, revision.revid, revision.url, revision.user, revision.userid, revision.comment,))\n","\n","  print('Analysing communities of similarity...')\n","  for keyphrase in keyphrases:\n","    df = pd.DataFrame(contexts[keyphrase], columns=['revindx','timestamp','left_id','left','keyphrase','right','right_id','revid','url','user','userid','comment',])\n","    # determine communities of similarity for left and right contexts\n","    for side in ['left', 'right']:\n","      G = nx.Graph()\n","      unique_contexts = list(df[side].unique())\n","      length = len(unique_contexts)\n","      for query in unique_contexts:\n","        indx = unique_contexts.index(query) + 1\n","        if indx <= length:\n","          scores = process.extractBests(query, unique_contexts[unique_contexts.index(query) + 1:], score_cutoff=score_cutoff, limit=length, scorer=scorer) # https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/process.py\n","          edges = [(query, i[0]) for i in scores]\n","          G.add_edges_from(edges)\n","      # add community_ids for lonesome contexts as well\n","      unique_contexts = set(unique_contexts)\n","      community_dic = community_louvain.best_partition(G)\n","      if community_dic:\n","        community_contexts = set(community_dic.keys())\n","        lonesome_contexts = unique_contexts - community_contexts\n","        lonesome_dic = {k:v for v,k in enumerate(lonesome_contexts,start=max(community_dic.values())+1)}\n","        community_dic.update(lonesome_dic) # hiermit nun 'lid' und 'rid' Spalten befüllen...\n","      else:\n","        community_dic = {k:v for v,k in enumerate(unique_contexts)}\n","      # add community_ids to df\n","      column = df.apply(lambda row: community_dic[row[side]], axis=1)\n","      df['{}_id'.format(side)] = column\n","      df.sort_values(by=['left_id','timestamp'])\n","      if excel_path:\n","        df.to_excel(excel_path + '/contexts_{}_{}.xlsx'.format(article.name, keyphrase))\n","      dfs[keyphrase] = df\n","  return dfs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QliI0Pt7eQ3"},"source":["### Define keyphrases"]},{"cell_type":"code","metadata":{"id":"vR8zSAmK3m5y"},"source":["keyphrases = [  \n","                                    \n","                # discoveries\n","                'first',\n","                'discover',\n","                \n","                # naming things\n","                'name', # CRISPR acronym\n","                'acronym', # CRISPR acronym\n","                \n","                # patents\n","                'patent',\n","                'property',\n","                'right',\n","\n","                # clinical trials\n","                'trial',\n","                'clinical',\n","                'patient',\n","                'approve', # e.g. FDA approved...\n","\n","                # awards\n","                'award',\n","                'prize',\n","                'nobel',\n","\n","                # people:\n","                'doudna', \n","                'charpentier', \n","                'zhang', \n","                'šikšnys', # with all accents\n","                  'šiksnys', # only first accent\n","                  'sikšnys', # only second accent\n","                  'siksnys', # no accents\n","                'gašiūnas', # with all accents\n","                  'gasiūnas', # only first accent\n","                  'gašiunas', # only second accent\n","                  'gasiunas', # no accents\n","                'horvath', # auch im Zusammenhang mit Šikšnys, aber auch so interessant\n","                'barrangou', # auch im Zusammenhang mit Šikšnys, aber auch so interessant\n","                'church',\n","                'brouns',\n","                'mojica',\n","                'ishino',\n","                'jansen',\n","                'marraffini',\n","                'sontheimer',\n","                'koonin',\n","                'moineau',\n","                'bolotin',\n","                'sorokin',\n","                'makarova',\n","                'pourcel',\n","                'overbeek',\n","                'liang',\n","                ' xu', # added space!\n","                'jiankui',\n","                'nishimasu',\n","                'shibata',\n","                'lander', # review\n","                \n","                # institutions:\n","                'osaka',\n","                'alicante',\n","                'danisco',\n","                'broad',\n","                'harvard',\n","                'university of california', \n","                'berkeley',\n","                \n","                ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2wVTNRY7hB_"},"source":["### Iterate over articles and apply function (main loop)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkTvZeIe3bdv","executionInfo":{"status":"ok","timestamp":1610661547061,"user_tz":-60,"elapsed":7408,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"66c042c4-bb64-4da8-c324-2d76fb8b3b44"},"source":["for article in [\n","                # CRISPR_en,\n","                # CRISPR_gene_editing_en,\n","                ]:\n","  print(article.name)\n","  keyphrases_in_context(keyphrases, article, width=100, score_cutoff=75, excel_path=path_extractions+'/keyphrase_contexts')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CRISPR_gene_editing_en\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PgVJr7mPzwj_"},"source":["# Old stuff..."]},{"cell_type":"markdown","metadata":{"id":"DP2I4yzW0vl2"},"source":["### Old approach"]},{"cell_type":"code","metadata":{"id":"eLjQwf0rnLmi"},"source":["def keyphrase_in_context(keyphrase, article, width=50, score_cutoff=75, scorer=fuzz.ratio):\n","  ''' \n","  Returns df for contexts to the left and to the right of keyphrase\n","  '''\n","  gen = article.yield_revisions()\n","  contexts = []\n","  for revision in gen:\n","    text = revision.get_text()\n","    text = text.lower()\n","    if keyphrase in text:\n","      for indx in find_all(text, keyphrase):\n","        # context left\n","        left = ''\n","        for char in text[indx - width : indx][::-1]:\n","          if char == '\\n': # make '\\n' the boundary for context\n","            break\n","          left += char\n","        left = left[::-1].strip()\n","        # context right\n","        right = ''\n","        for char in text[indx + len(keyphrase): indx + len(keyphrase) + width]:\n","          if char == '\\n': # make '\\n' the boundary for context\n","            break\n","          right += char\n","        right = right.strip()\n","        contexts.append((revision.index, revision.timestamp.string, 0, left, keyphrase, right, 0, revision.revid, revision.url, revision.user, revision.userid, revision.comment,))\n","  df = pd.DataFrame(contexts, columns=['revindx','timestamp','left_id','left','keyphrase','right','right_id','revid','url','user','userid','comment',])\n","  \n","  # determine communities of similarity for left and right contexts\n","  for side in ['left', 'right']:\n","    G = nx.Graph()\n","    unique_contexts = list(df[side].unique())\n","    length = len(unique_contexts)\n","    for query in unique_contexts:\n","      indx = unique_contexts.index(query) + 1\n","      if indx <= length:\n","        scores = process.extractBests(query, unique_contexts[unique_contexts.index(query) + 1:], score_cutoff=score_cutoff, limit=length, scorer=scorer) # https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/process.py\n","        edges = [(query, i[0]) for i in scores]\n","        G.add_edges_from(edges)\n","    # add community_ids for lonesome contexts as well\n","    unique_contexts = set(unique_contexts)\n","    community_dic = community_louvain.best_partition(G)\n","    if community_dic:\n","      community_contexts = set(community_dic.keys())\n","      lonesome_contexts = unique_contexts - community_contexts\n","      lonesome_dic = {k:v for v,k in enumerate(lonesome_contexts,start=max(community_dic.values())+1)}\n","      community_dic.update(lonesome_dic) # hiermit nun 'lid' und 'rid' Spalten befüllen...\n","    else:\n","      community_dic = {k:v for v,k in enumerate(unique_contexts)}\n","    # add community_ids to df\n","    column = df.apply(lambda row: community_dic[row[side]], axis=1)\n","    df['{}_id'.format(side)] = column\n","  return df\n","  # # add community_ids for lonesome contexts as well\n","  # unique_contexts = set(unique_contexts['left'] + unique_contexts['right'])\n","  # community_dic = community_louvain.best_partition(G)\n","  # community_contexts = set(community_dic.keys())\n","  # lonesome_contexts = unique_contexts - community_contexts\n","  # lonesome_dic = {k:v for v,k in enumerate(lonesome_contexts,start=max(community_dic.values())+1)}\n","  # community_dic.update(lonesome_dic) # hiermit nun 'lid' und 'rid' Spalten befüllen...\n","  # # add community_ids for all left and right contexts in df\n","  # for side in ['left', 'right']:\n","  #   column = df.apply(lambda row: community_dic[row[side]], axis=1)\n","  #   df['{}_id'.format(side)] = column\n","  # return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414},"id":"ghiJjtnuvUIB","executionInfo":{"status":"error","timestamp":1610559404039,"user_tz":-60,"elapsed":1479,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"7fd0caa7-a102-4eaa-97f5-19e7cd919bc7"},"source":["for article, article_name in [\n","                # (CRISPR_en, 'CRISPR_en'),\n","                # (CRISPR_gene_editing_en, 'CRISPR_gene_editing_en'),\n","                ]:\n","  print(article_name)\n","  for keyphrase in [\n","                                    \n","                # discoveries\n","                'first',\n","                'discover',\n","                \n","                # naming things\n","                'name', # CRISPR acronym\n","                'acronym', # CRISPR acronym\n","                \n","                # patents\n","                'patent',\n","                'property',\n","                'right',\n","\n","                # clinical trials\n","                'trial',\n","                'clinical',\n","                'patient',\n","                'approve', # e.g. FDA approved...\n","\n","                # awards\n","                'award',\n","                'prize',\n","                'nobel',\n","\n","                # people:\n","                'doudna', \n","                'charpentier', \n","                'zhang', \n","                'šikšnys', # with all accents\n","                  'šiksnys', # only first accent\n","                  'sikšnys', # only second accent\n","                  'siksnys', # no accents\n","                'gašiūnas', # with all accents\n","                  'gasiūnas', # only first accent\n","                  'gašiunas', # only second accent\n","                  'gasiunas', # no accents\n","                'horvath', # auch im Zusammenhang mit Šikšnys, aber auch so interessant\n","                'barrangou', # auch im Zusammenhang mit Šikšnys, aber auch so interessant\n","                'church',\n","                'brouns',\n","                'mojica',\n","                'ishino',\n","                'jansen',\n","                'marraffini',\n","                'sontheimer',\n","                'koonin',\n","                'moineau',\n","                'bolotin',\n","                'sorokin',\n","                'makarova',\n","                'pourcel',\n","                'overbeek',\n","                'liang',\n","                ' xu', # added space!\n","                'jiankui',\n","                'nishimasu',\n","                'shibata',\n","                'lander', # review\n","                \n","                # institutions:\n","                'osaka',\n","                'alicante',\n","                'danisco',\n","                'broad',\n","                'harvard',\n","                'university of california', \n","                'berkeley',\n","                \n","                ]:\n","    print('\\t' + keyphrase)\n","    df = keyphrase_in_context(keyphrase, article, width=100, score_cutoff=75)\n","    df.sort_values(by=['left_id','timestamp'])\n","    df.to_excel(path_extractions + '/contexts_{}_{}.xlsx'.format(article_name, keyphrase))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CRISPR_gene_editing_en\n","\tarno\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-4d864295b87a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m                 ]:\n\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeyphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyphrase_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyphrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_cutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# df.to_excel(path_extractions + '/contexts_{}_{}.xlsx'.format(article_name, keyphrase))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-6d40ff7cce69>\u001b[0m in \u001b[0;36mkeyphrase_in_context\u001b[0;34m(keyphrase, article, width, score_cutoff, scorer)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myield_revisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mrevision\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/WIKING/WIKING_git/code/article/article.py\u001b[0m in \u001b[0;36myield_revisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mrevision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mRevision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrack_field_values_in_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbibliography\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'wikitext'"]}]},{"cell_type":"markdown","metadata":{"id":"4k2Me_0RyhFO"},"source":["### CRISPR_en: \"first\"\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":788},"id":"CPm0Mh-syhFP","executionInfo":{"status":"ok","timestamp":1606682187152,"user_tz":-60,"elapsed":97739,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"4885e2f4-70b3-4a97-da4a-cf6c8843f258"},"source":["df = keyphrase_in_context('first', CRISPR_en, width=50, score_cutoff=75)\n","df.to_excel(file_contexts_CRISPR_en)\n","df.sort_values(by=['left_id','timestamp'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>revindx</th>\n","      <th>timestamp</th>\n","      <th>left_id</th>\n","      <th>left</th>\n","      <th>keyphrase</th>\n","      <th>right</th>\n","      <th>right_id</th>\n","      <th>revid</th>\n","      <th>url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>25</th>\n","      <td>133</td>\n","      <td>2012-01-07 04:36:56</td>\n","      <td>0</td>\n","      <td>n of the crispr-cas system in 2012[21] provided a</td>\n","      <td>first</td>\n","      <td>step toward realization of some of the several pr</td>\n","      <td>33</td>\n","      <td>470025144</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>134</td>\n","      <td>2012-08-01 19:27:33</td>\n","      <td>0</td>\n","      <td>n of the crispr-cas system in 2012[21] provided a</td>\n","      <td>first</td>\n","      <td>step toward realization of some of the several pr</td>\n","      <td>33</td>\n","      <td>505309163</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>135</td>\n","      <td>2012-08-22 21:44:35</td>\n","      <td>0</td>\n","      <td>n of the crispr-cas system in 2012[21] provided a</td>\n","      <td>first</td>\n","      <td>step toward realization of some of the several pr</td>\n","      <td>33</td>\n","      <td>508685743</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>136</td>\n","      <td>2012-12-06 19:12:03</td>\n","      <td>0</td>\n","      <td>n of the crispr-cas system in 2012[21] provided a</td>\n","      <td>first</td>\n","      <td>step toward realization of some of the several pr</td>\n","      <td>33</td>\n","      <td>526751001</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>137</td>\n","      <td>2012-12-11 08:13:19</td>\n","      <td>0</td>\n","      <td>n of the crispr-cas system in 2012[21] provided a</td>\n","      <td>first</td>\n","      <td>step toward realization of some of the several pr</td>\n","      <td>33</td>\n","      <td>527490124</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>26284</th>\n","      <td>2039</td>\n","      <td>2020-11-02 21:24:41</td>\n","      <td>75</td>\n","      <td>th the newly acquired spacer inserted between the</td>\n","      <td>first</td>\n","      <td>and second direct repeats.[95][116]</td>\n","      <td>5</td>\n","      <td>986762186</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>26297</th>\n","      <td>2040</td>\n","      <td>2020-11-03 09:53:58</td>\n","      <td>75</td>\n","      <td>th the newly acquired spacer inserted between the</td>\n","      <td>first</td>\n","      <td>and second direct repeats.[95][116]</td>\n","      <td>5</td>\n","      <td>986842914</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>26310</th>\n","      <td>2041</td>\n","      <td>2020-11-03 20:16:34</td>\n","      <td>75</td>\n","      <td>th the newly acquired spacer inserted between the</td>\n","      <td>first</td>\n","      <td>and second direct repeats.[95][116]</td>\n","      <td>5</td>\n","      <td>986927023</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>26323</th>\n","      <td>2042</td>\n","      <td>2020-11-03 22:56:29</td>\n","      <td>75</td>\n","      <td>th the newly acquired spacer inserted between the</td>\n","      <td>first</td>\n","      <td>and second direct repeats.[95][116]</td>\n","      <td>5</td>\n","      <td>986949106</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>26336</th>\n","      <td>2043</td>\n","      <td>2020-11-04 21:00:44</td>\n","      <td>75</td>\n","      <td>th the newly acquired spacer inserted between the</td>\n","      <td>first</td>\n","      <td>and second direct repeats.[95][116]</td>\n","      <td>5</td>\n","      <td>987088136</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>26340 rows × 9 columns</p>\n","</div>"],"text/plain":["       revindx  ...                                                url\n","25         133  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","27         134  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","29         135  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","31         136  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","33         137  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","...        ...  ...                                                ...\n","26284     2039  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","26297     2040  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","26310     2041  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","26323     2042  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","26336     2043  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","\n","[26340 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"fc1dPCidx823"},"source":["### CRISPR_gene_editing_en: \"first\"\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":771},"id":"dB8a9egkrWyg","executionInfo":{"status":"ok","timestamp":1606681779613,"user_tz":-60,"elapsed":6031,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"508e83c7-d9e5-4ccc-8689-f495287716c9"},"source":["df = keyphrase_in_context('first', CRISPR_gene_editing_en, width=50, score_cutoff=75)\n","df.to_excel(file_contexts_CRISPR_gene_editing_en)\n","df.sort_values(by=['left_id','timestamp'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>revindx</th>\n","      <th>timestamp</th>\n","      <th>left_id</th>\n","      <th>left</th>\n","      <th>keyphrase</th>\n","      <th>right</th>\n","      <th>right_id</th>\n","      <th>revid</th>\n","      <th>url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>2019-02-17 06:32:48</td>\n","      <td>0</td>\n","      <td>340.2.[131] yang's white button mushroom was the</td>\n","      <td>first</td>\n","      <td>organism genetically modified with the crispr/cas</td>\n","      <td>3</td>\n","      <td>883727671</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>2019-02-17 06:33:54</td>\n","      <td>0</td>\n","      <td>340.2.[131] yang's white button mushroom was the</td>\n","      <td>first</td>\n","      <td>organism genetically modified with the crispr/cas</td>\n","      <td>3</td>\n","      <td>883727741</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2</td>\n","      <td>2019-02-17 06:35:41</td>\n","      <td>0</td>\n","      <td>340.2.[131] yang's white button mushroom was the</td>\n","      <td>first</td>\n","      <td>organism genetically modified with the crispr/cas</td>\n","      <td>3</td>\n","      <td>883727860</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>3</td>\n","      <td>2019-02-17 06:37:14</td>\n","      <td>0</td>\n","      <td>340.2.[132] yang's white button mushroom was the</td>\n","      <td>first</td>\n","      <td>organism genetically modified with the crispr/cas</td>\n","      <td>3</td>\n","      <td>883727959</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>4</td>\n","      <td>2019-02-17 06:39:37</td>\n","      <td>0</td>\n","      <td>340.2.[131] yang's white button mushroom was the</td>\n","      <td>first</td>\n","      <td>organism genetically modified with the crispr/cas</td>\n","      <td>3</td>\n","      <td>883728113</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1447</th>\n","      <td>184</td>\n","      <td>2020-10-23 09:36:41</td>\n","      <td>13</td>\n","      <td>ay:table-row}.mw-parser-output .portal&gt;ul&gt;li&gt;s...</td>\n","      <td>first</td>\n","      <td>-child{display:table-cell;padding:0.2em;vertic...</td>\n","      <td>15</td>\n","      <td>984993980</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>1458</th>\n","      <td>185</td>\n","      <td>2020-11-02 04:29:36</td>\n","      <td>13</td>\n","      <td>ay:table-row}.mw-parser-output .portal&gt;ul&gt;li&gt;s...</td>\n","      <td>first</td>\n","      <td>-child{display:table-cell;padding:0.2em;vertic...</td>\n","      <td>15</td>\n","      <td>986649176</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>1469</th>\n","      <td>186</td>\n","      <td>2020-11-02 10:34:00</td>\n","      <td>13</td>\n","      <td>ay:table-row}.mw-parser-output .portal&gt;ul&gt;li&gt;s...</td>\n","      <td>first</td>\n","      <td>-child{display:table-cell;padding:0.2em;vertic...</td>\n","      <td>15</td>\n","      <td>986682078</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>1480</th>\n","      <td>187</td>\n","      <td>2020-11-02 10:35:18</td>\n","      <td>13</td>\n","      <td>ay:table-row}.mw-parser-output .portal&gt;ul&gt;li&gt;s...</td>\n","      <td>first</td>\n","      <td>-child{display:table-cell;padding:0.2em;vertic...</td>\n","      <td>15</td>\n","      <td>986682164</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","    <tr>\n","      <th>1491</th>\n","      <td>188</td>\n","      <td>2020-11-02 10:35:49</td>\n","      <td>13</td>\n","      <td>ay:table-row}.mw-parser-output .portal&gt;ul&gt;li&gt;s...</td>\n","      <td>first</td>\n","      <td>-child{display:table-cell;padding:0.2em;vertic...</td>\n","      <td>15</td>\n","      <td>986682206</td>\n","      <td>https://en.wikipedia.org/w/index.php?title=CRI...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1497 rows × 9 columns</p>\n","</div>"],"text/plain":["      revindx  ...                                                url\n","3           0  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","10          1  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","17          2  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","24          3  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","31          4  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","...       ...  ...                                                ...\n","1447      184  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","1458      185  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","1469      186  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","1480      187  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","1491      188  ...  https://en.wikipedia.org/w/index.php?title=CRI...\n","\n","[1497 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"6QRPtrvO9Mvp"},"source":["### Occurrence spans"]},{"cell_type":"code","metadata":{"id":"PrlDj91nsCLY"},"source":["def occurance_spans(keyphrase, article):\n","  ''' \n","  returns a list of time spans during which a keyphrase 'keyphrase' continously appears in an article 'a'\n","  '''\n","  gen = article.yield_revisions()\n","  last_rev_was_match = False\n","  spans = []\n","  for revision in gen:\n","    if keyphrase in revision.get_text():\n","    # if keyphrase in i.get_text().split('\\nLocus structure\\n')[0]: # Trick 17 to exclude everything after history section and practially only search in histroy section\n","      last_rev_that_matched = revision\n","      if not last_rev_was_match:\n","        span_beg = revision\n","        last_rev_was_match = True\n","    else:\n","      if last_rev_was_match:\n","        spans.append((span_beg, last_rev_that_matched))\n","        last_rev_was_match = False\n","  if last_rev_was_match:\n","    spans.append((span_beg, last_rev_that_matched))\n","  print(\n","      '\"{}\" was found in the following revisions of {}:\\n{}'.format(keyphrase, 'Test', '\\n'.join(['Span {}: {} - {}'.format(indx + 1, revision[0].timestamp, revision[1].timestamp) for indx,revision in enumerate(spans)])) \n","      if spans else '\"{}\" was not found in any revision of \"{}\"'.format(keyphrase, 'Test')\n","      )\n","  return spans"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y5HIHBrA9Sak"},"source":["### Unique contexts"]},{"cell_type":"code","metadata":{"id":"_eUn_KzFheAn"},"source":["def unique_contexts(keyphrase, article, width=50):\n","  ''' \n","  returns a pandas df with three columns:\n","  1) unique contexts of width in which keyphrase appears in article\n","  2) timestamp for first occurrence of each context\n","  3) url for first occurrence of each context\n","  '''\n","  kill = re.compile(r'\\s*\\[.*?\\]')\n","  gen = article.yield_revisions()\n","  unique_contexts = []\n","  timestamps = []\n","  urls = []\n","  for revision in gen:\n","    text = revision.get_text()\n","    # if s in text.split('\\nLocus structure\\n')[0]: # Trick 17 to exclude everything after history section and practially only search in histroy section\n","    if keyphrase in text:\n","      text = kill.sub('', text) # kill intext refs like '[23]'\n","      text = text.lower()\n","      for indx in find_all(text, keyphrase):\n","        # make '/n' the boundary for context\n","        before = text[indx - width : indx]\n","        after = text[indx + len(keyphrase): indx + len(keyphrase) + width]\n","        context = keyphrase[::-1]\n","        for char in before[::-1]:\n","          if char == '\\n':\n","            break\n","          context += char\n","        context = context[::-1]\n","        for char in after:\n","          if char == '\\n':\n","            break\n","          context += char\n","        # add unique contexts and add their meta data\n","        context = context.strip()\n","        if not context in unique_contexts:\n","          unique_contexts.append(context)\n","          timestamps.append(revision.timestamp.string)\n","          urls.append(revision.url)    \n","  print('Found {} unique contexts for \"{}\" in article \"{}\".'.format(len(unique_contexts), keyphrase, article.name))\n","  # return list(zip(unique_contexts, timestamps, urls))\n","  return pd.DataFrame({'Context':unique_contexts, \n","        'Timestamp':timestamps,\n","        'Url':urls}) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajWDEbMWHHSw"},"source":["### Left-right contexts"]},{"cell_type":"code","metadata":{"id":"7elYtyzQr-PP"},"source":["def lr_context_graph(keyphrase, article, width=50):\n","  ''' \n","  Returns networkx DiGraph for links between contexts to the left and to the right of keyphrase\n","  '''\n","  gen = article.yield_revisions()\n","  DG = nx.DiGraph(keyphrase=keyphrase)\n","  for revision in gen:\n","    text = revision.get_text()\n","    text = text.lower()\n","    # text = re.sub(r'\\s*\\[.*?\\]','', text) # kill intext refs like '[23]'\n","    metadata = (revision.timestamp.string, revision.url)\n","    if keyphrase in text:\n","      for indx in find_all(text, keyphrase):\n","        # context left\n","        left = ''\n","        for char in text[indx - width : indx][::-1]:\n","          if char == '\\n': # make '/n' the boundary for context\n","            break\n","          left += char\n","        left = left[::-1].strip()\n","        # add/update node for context left\n","        if not left in DG:\n","          DG.add_node(left, as_left=[metadata], as_right=[])\n","        else:\n","          DG.nodes[left]['as_left'].append(metadata)\n","        # context right\n","        right = ''\n","        for char in text[indx + len(keyphrase): indx + len(keyphrase) + width]:\n","          if char == '\\n': # make '/n' the boundary for context\n","            break\n","          right += char\n","        right = right.strip()\n","        # add/update node for context right\n","        if not right in DG:\n","          DG.add_node(right, as_right=[metadata], as_left=[])\n","        else:\n","          DG.nodes[right]['as_right'].append(metadata)\n","        # add/update edge from context left to context right\n","        if not [left, right] in DG.edges:\n","          DG.add_edge(left, right, occurrences=[metadata])\n","        else:\n","            DG.edges[left,right]['occurrences'].append(metadata)\n","  return DG"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3QeZcjHpN83"},"source":["### Node communities"]},{"cell_type":"code","metadata":{"id":"4cJ9mGfolzDZ"},"source":["def node_community_dic(DG, score_cutoff=90, scorer=fuzz.ratio):\n","  keyphrase = DG.graph['keyphrase']\n","  G = nx.Graph()\n","  for side in ['left', 'right']:\n","    choices = [node[0] for node in DG.nodes.data() if node[1]['as_{}'.format(side)]]\n","    length = len(choices)\n","    for query in choices:\n","      indx = choices.index(query) + 1\n","      if indx <= length:\n","        scores = process.extractBests(query, choices[choices.index(query) + 1:], score_cutoff=score_cutoff, limit=length, scorer=scorer) # https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/process.py\n","        edges = [(query, i[0]) for i in scores]\n","        G.add_edges_from(edges)\n","  return community_louvain.best_partition(G)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTaYKRvg6hN-"},"source":["### Master node for each community"]},{"cell_type":"code","metadata":{"id":"ATQugc1nqHFy"},"source":["def master_nodes_dic(DG, ncd):\n","  '''\n","  Returns dictionary with community_ids as keys and nodes + latest occurrence as value\n","  '''\n","  result = {}\n","  reverse_mnd = {v:[k for k in mnd if mnd[k] == v] for v in mnd.values()}\n","  for community_id, nodes in reverse_mnd.items():\n","    candidates = []\n","    for node in nodes:\n","      latest = max([DG.nodes[node]['as_{}'.format(side)][-1][0] for side in ['left', 'right'] if DG.nodes[node]['as_{}'.format(side)]])\n","      candidates.append((node, latest))\n","    winner = sorted(candidates, key= lambda x: (x[1], x[0]), reverse=True)[0]\n","    result[community_id] = winner\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpfcElE48O3W"},"source":["### Reduced LR contexts graph (using ``lr_context_graph``, ``node_community_dic``, and ``master_nodes_dic``)"]},{"cell_type":"code","metadata":{"id":"wgGSr0Fl8ii7"},"source":["def reduce_lr_context_graph(DG, ncd, mnd):\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bYzX8wKFrxkB"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"5pFtlIlNiYtw"},"source":["CRISPR_en = Article(file_CRISPR_en)\n","CRISPR_gene_editing_en = Article(file_CRISPR_gene_editing_en)\n","\n","# events = pd.read_csv(file_events)\n","# accounts = pd.read_csv(file_accounts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iN1eI6MICMu_"},"source":["## Experiments"]},{"cell_type":"markdown","metadata":{"id":"CPg3sesxo00n"},"source":["### Extract LR contexts and merge nodes "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZAA1h3_wuO3D","executionInfo":{"status":"ok","timestamp":1606500810339,"user_tz":-60,"elapsed":615,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"6b665a2a-d41b-4e75-9d90-9da12af436f8"},"source":["DG = lr_context_graph('first', CRISPR_gene_editing_en, width=50)\n","ncd = node_community_dic(DG)\n","# sorted(ncd.items(), key=lambda item:(item[1], item[0]))\n","mnd = master_nodes_dic(DG, ncd)\n","mnd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: (\"340.2.[145] yang's white button mushroom was the\",\n","  '2020-11-02 10:35:49'),\n"," 1: ('t position 8 (help); no-break space character in |',\n","  '2020-01-31 21:33:44'),\n"," 2: ('demonstrated in 2012 in:[25][26]', '2019-02-18 01:48:30'),\n"," 3: ('organism genetically modified with the crispr-cas',\n","  '2020-11-02 10:35:49'),\n"," 4: ('= at position 5 (help); no-break space character i',\n","  '2020-01-31 21:33:44')}"]},"metadata":{"tags":[]},"execution_count":255}]},{"cell_type":"markdown","metadata":{"id":"IJppJpuY6ZAF"},"source":["### Older stuff"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_vRSWezNP31","executionInfo":{"status":"ok","timestamp":1606423116262,"user_tz":-60,"elapsed":76291,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"fe9f6fea-2e38-4840-ab2c-d17b6f5b9009"},"source":["contexts_first = unique_contexts('first', CRISPR_en, width=50)\n","contexts_first.to_excel(file_contexts_CRISPR_en)\n","\n","contexts_first = unique_contexts('first', CRISPR_gene_editing_en, width=50)\n","contexts_first.to_excel(file_contexts_CRISPR_gene_editing_en)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 143 unique contexts for \"first\" in article \"CRISPR_en\".\n","Found 23 unique contexts for \"first\" in article \"CRISPR_gene_editing_en\".\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yJguDbiEIxKv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606410107435,"user_tz":-60,"elapsed":85293,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"a30c97b9-40ce-41d7-9431-988943b84a8a"},"source":["timeline = occurance_spans('CRISPR Timeline', CRISPR_en)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"CRISPR Timeline\" was found in the following revisions of Test:\n","Span 1: {'datetime': datetime.datetime(2016, 9, 28, 9, 26, 29),\n"," 'day': 28,\n"," 'hour': 9,\n"," 'minute': 26,\n"," 'month': 9,\n"," 'second': 29,\n"," 'string': '2016-09-28 09:26:29',\n"," 'year': 2016} - {'datetime': datetime.datetime(2016, 12, 28, 8, 5, 49),\n"," 'day': 28,\n"," 'hour': 8,\n"," 'minute': 5,\n"," 'month': 12,\n"," 'second': 49,\n"," 'string': '2016-12-28 08:05:49',\n"," 'year': 2016}\n","Span 2: {'datetime': datetime.datetime(2019, 3, 18, 14, 33, 42),\n"," 'day': 18,\n"," 'hour': 14,\n"," 'minute': 33,\n"," 'month': 3,\n"," 'second': 42,\n"," 'string': '2019-03-18 14:33:42',\n"," 'year': 2019} - {'datetime': datetime.datetime(2019, 3, 18, 14, 34, 29),\n"," 'day': 18,\n"," 'hour': 14,\n"," 'minute': 34,\n"," 'month': 3,\n"," 'second': 29,\n"," 'string': '2019-03-18 14:34:29',\n"," 'year': 2019}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MtcXB1nBIkkq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604323635576,"user_tz":-60,"elapsed":74224,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"5367bc1b-4e68-442c-e636-2702ceb5a7c9"},"source":["# history_trick17 = occurance_spans('\\nLocus structure\\n', CRISPR_en)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"\n","Locus structure\n","\" was not found in any revision of \"Test\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R2nt3LFoDezQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604322549498,"user_tz":-60,"elapsed":78372,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"439e1385-bad6-41a0-e927-2043fe561133"},"source":["history = occurance_spans('\\nLocus structure\\n', CRISPR_en)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"\n","Locus structure\n","\" was found in the following revisions of Test:\n","Span 1: {'datetime': datetime.datetime(2013, 11, 24, 21, 3, 53),\n"," 'day': 24,\n"," 'hour': 21,\n"," 'minute': 3,\n"," 'month': 11,\n"," 'second': 53,\n"," 'string': '2013-11-24 21:03:53',\n"," 'year': 2013} - {'datetime': datetime.datetime(2016, 9, 22, 22, 1, 43),\n"," 'day': 22,\n"," 'hour': 22,\n"," 'minute': 1,\n"," 'month': 9,\n"," 'second': 43,\n"," 'string': '2016-09-22 22:01:43',\n"," 'year': 2016}\n","Span 2: {'datetime': datetime.datetime(2016, 9, 23, 0, 51, 24),\n"," 'day': 23,\n"," 'hour': 0,\n"," 'minute': 51,\n"," 'month': 9,\n"," 'second': 24,\n"," 'string': '2016-09-23 00:51:24',\n"," 'year': 2016} - {'datetime': datetime.datetime(2017, 9, 18, 8, 58, 26),\n"," 'day': 18,\n"," 'hour': 8,\n"," 'minute': 58,\n"," 'month': 9,\n"," 'second': 26,\n"," 'string': '2017-09-18 08:58:26',\n"," 'year': 2017}\n","Span 3: {'datetime': datetime.datetime(2017, 9, 18, 9, 0, 40),\n"," 'day': 18,\n"," 'hour': 9,\n"," 'minute': 0,\n"," 'month': 9,\n"," 'second': 40,\n"," 'string': '2017-09-18 09:00:40',\n"," 'year': 2017} - {'datetime': datetime.datetime(2018, 9, 24, 16, 48, 11),\n"," 'day': 24,\n"," 'hour': 16,\n"," 'minute': 48,\n"," 'month': 9,\n"," 'second': 11,\n"," 'string': '2018-09-24 16:48:11',\n"," 'year': 2018}\n","Span 4: {'datetime': datetime.datetime(2018, 9, 29, 6, 55, 25),\n"," 'day': 29,\n"," 'hour': 6,\n"," 'minute': 55,\n"," 'month': 9,\n"," 'second': 25,\n"," 'string': '2018-09-29 06:55:25',\n"," 'year': 2018} - {'datetime': datetime.datetime(2018, 9, 29, 6, 55, 25),\n"," 'day': 29,\n"," 'hour': 6,\n"," 'minute': 55,\n"," 'month': 9,\n"," 'second': 25,\n"," 'string': '2018-09-29 06:55:25',\n"," 'year': 2018}\n","Span 5: {'datetime': datetime.datetime(2018, 9, 30, 12, 48, 13),\n"," 'day': 30,\n"," 'hour': 12,\n"," 'minute': 48,\n"," 'month': 9,\n"," 'second': 13,\n"," 'string': '2018-09-30 12:48:13',\n"," 'year': 2018} - {'datetime': datetime.datetime(2019, 2, 28, 15, 35, 54),\n"," 'day': 28,\n"," 'hour': 15,\n"," 'minute': 35,\n"," 'month': 2,\n"," 'second': 54,\n"," 'string': '2019-02-28 15:35:54',\n"," 'year': 2019}\n","Span 6: {'datetime': datetime.datetime(2019, 2, 28, 15, 38, 47),\n"," 'day': 28,\n"," 'hour': 15,\n"," 'minute': 38,\n"," 'month': 2,\n"," 'second': 47,\n"," 'string': '2019-02-28 15:38:47',\n"," 'year': 2019} - {'datetime': datetime.datetime(2019, 5, 20, 16, 37, 18),\n"," 'day': 20,\n"," 'hour': 16,\n"," 'minute': 37,\n"," 'month': 5,\n"," 'second': 18,\n"," 'string': '2019-05-20 16:37:18',\n"," 'year': 2019}\n","Span 7: {'datetime': datetime.datetime(2019, 5, 21, 17, 37, 36),\n"," 'day': 21,\n"," 'hour': 17,\n"," 'minute': 37,\n"," 'month': 5,\n"," 'second': 36,\n"," 'string': '2019-05-21 17:37:36',\n"," 'year': 2019} - {'datetime': datetime.datetime(2020, 10, 7, 14, 20, 24),\n"," 'day': 7,\n"," 'hour': 14,\n"," 'minute': 20,\n"," 'month': 10,\n"," 'second': 24,\n"," 'string': '2020-10-07 14:20:24',\n"," 'year': 2020}\n","Span 8: {'datetime': datetime.datetime(2020, 10, 8, 1, 22, 24),\n"," 'day': 8,\n"," 'hour': 1,\n"," 'minute': 22,\n"," 'month': 10,\n"," 'second': 24,\n"," 'string': '2020-10-08 01:22:24',\n"," 'year': 2020} - {'datetime': datetime.datetime(2020, 10, 10, 21, 4, 18),\n"," 'day': 10,\n"," 'hour': 21,\n"," 'minute': 4,\n"," 'month': 10,\n"," 'second': 18,\n"," 'string': '2020-10-10 21:04:18',\n"," 'year': 2020}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XPiKH4pzvbRs","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1603732761615,"user_tz":-60,"elapsed":937,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"5bed8eb7-3188-4821-db88-9ab8092531c6"},"source":["mojica = occurance_spans('Mojica', CRISPR_en)\n","# for span in doudna:\n","#   print(span[0].timestamp, span[1].timestamp)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\"Mojica\" was found in the following revisions of Test:\n","Span 1: 2010-01-11T02:11:54Z - 2020-09-02T08:32:27Z\n"],"name":"stdout"}]}]}