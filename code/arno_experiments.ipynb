{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"arno_experiments.ipynb","provenance":[{"file_id":"1IK-QF90VpGAZmfH0XqDrD8doJfuC6B92","timestamp":1610636648085}],"collapsed_sections":["wSZjc1g51so2","BG3rjcwdsAvH","4rG5Bsw18IoO"],"authorship_tag":"ABX9TyPZ0IaAYUM/I3R6uIcIlqhk"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n4eNz7RIrzow"},"source":["## Set globals"]},{"cell_type":"code","metadata":{"id":"PJwyfS7eiPWw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610997164268,"user_tz":-60,"elapsed":28870,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"d991c1c5-01ea-4822-dbec-ea0408978fb3"},"source":["# set paths\n","path_extractions = '/content/drive/My Drive/WIKING/extractions'\n","path_git = '/content/drive/My Drive/WIKING/WIKING_git'\n","path_my_tools = '/content/drive/My Drive/my_tools'\n","\n","# infiles\n","file_CRISPR_de = path_extractions + '/wikipedia_entries/CRISPR_de' # get latest file version here: https://files.webis.de/wikipedia-tracing-innovations/\n","file_CRISPR_en = path_extractions + '/wikipedia_entries/CRISPR_en' # get latest file version here: https://files.webis.de/wikipedia-tracing-innovations/\n","file_CRISPR_gene_editing_en = path_extractions + '/wikipedia_entries/CRISPR_gene_editing_en' # get latest file version here: https://files.webis.de/wikipedia-tracing-innovations/ \n","file_events = path_git + '/data/CRISPR_events - events.csv'\n","file_accounts = path_git + '/data/CRISPR_events - accounts.csv'\n","\n","# mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# add git folder to path variable\n","import sys\n","sys.path.append(path_git + '/code')\n","\n","# import Wolfgang's classes:\n","from article.article import Article\n","\n","# import other modules\n","!pip install fuzzywuzzy # https://github.com/seatgeek/fuzzywuzzy\n","from fuzzywuzzy import fuzz\n","from fuzzywuzzy import process\n","import networkx as nx\n","!pip install python-louvain\n","from community import community_louvain\n","import re\n","import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import itertools\n","import pprint\n","pp = pprint.PrettyPrinter(indent=4)\n","from lxml import etree\n","\n","# set working directory\n","os.chdir(path_git)\n","os.getcwd()\n","\n","# Load data\n","CRISPR_en = Article(file_CRISPR_en)\n","CRISPR_gene_editing_en = Article(file_CRISPR_gene_editing_en)\n","# events = pd.read_csv(file_events)\n","# accounts = pd.read_csv(file_accounts)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Collecting fuzzywuzzy\n","  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n","Installing collected packages: fuzzywuzzy\n","Successfully installed fuzzywuzzy-0.18.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n","  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"],"name":"stderr"},{"output_type":"stream","text":["Requirement already satisfied: python-louvain in /usr/local/lib/python3.6/dist-packages (0.15)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from python-louvain) (1.19.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from python-louvain) (2.5)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->python-louvain) (4.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNoNFtyF829U","executionInfo":{"status":"ok","timestamp":1610998420133,"user_tz":-60,"elapsed":687,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"64385313-a451-46d5-aca4-7380d2c0020c"},"source":["def get_lr_contexts(text, keyphrase, width=50, wikitext=False, lower=False, return_keyphrase=False):\n","        '''\n","        Generic function to get contexts left and right of keyphrase\n","        Returns 2-tuple with left and right contexts, 3-tuple including keyphrase if specified by 'return_keyphrase'\n","        Author: Arno Simons\n","        '''\n","        def find_start_indices(text, keyphrase): # https://stackoverflow.com/questions/4664850/how-to-find-all-occurrences-of-a-substring\n","            ''' \n","            Returns start indices for all matches of keyphrase \n","            '''\n","            start = 0\n","            while True:\n","              start = text.find(keyphrase, start)\n","              if start == -1: \n","                return\n","              yield start\n","              start += len(keyphrase) # use start += 1 to find overlapping matches\n","\n","        contexts = []\n","        # text = self.get_text() if not wikitext else self.get_wikitext()\n","        if lower:\n","            text = text.lower()\n","        for indx in find_start_indices(text, keyphrase): # use wikitext instead?\n","            # context left\n","            left = ''\n","            for char in text[indx - width if not width > indx else 0: indx][::-1]:\n","                if char == '\\n': # make '\\n' the boundary for context\n","                    break\n","                left += char\n","            left = left[::-1].strip()\n","            # context right\n","            right = ''\n","            print('RIGHT:', text[indx + len(keyphrase): indx + len(keyphrase) + width])\n","            for char in text[indx + len(keyphrase): indx + len(keyphrase) + width]:\n","              if char == '\\n': # make '\\n' the boundary for context\n","                break\n","              right += char\n","            right = right.strip()\n","            contexts.append((left, right) if not return_keyphrase else (left, keyphrase, right))\n","        return contexts\n","get_lr_contexts('This is a test, lets see what test happens with the test', 'test', width=50, lower=False)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["RIGHT: , lets see what test happens with the test\n","RIGHT:  happens with the test\n","RIGHT: \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('This is a', ', lets see what test happens with the test'),\n"," ('This is a test, lets see what', 'happens with the test'),\n"," ('is is a test, lets see what test happens with the', '')]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"ZQTi7BPa-TpL","executionInfo":{"status":"ok","timestamp":1610998369922,"user_tz":-60,"elapsed":556,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"811f94a6-93dd-4188-fa66-7f70bd89475e"},"source":["a = 3\n","b = 1\n","'0123456'[a-b if not b>a else 0:3]"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2'"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3JHagkL7kws","executionInfo":{"status":"ok","timestamp":1610998470667,"user_tz":-60,"elapsed":558,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"26a1fee4-13d0-405b-8d38-d15c6b5ef250"},"source":["def get_lr_contexts(text, keyphrase, width=50, lower=False):\n","        '''\n","        Generic function to get contexts left and right of keyphrase\n","        Returns 2-tuple with left and right contexts, 3-tuple including keyphrase if specified by 'return_keyphrase'\n","        Author: Arno Simons\n","        '''\n","        contexts = []\n","        keyphrase = keyphrase.strip()\n","        if keyphrase:\n","            # text = self.get_text() if not lower else self.get_text().lower()\n","            for indx in [item.span()[0] for item in finditer(keyphrase, text)]:\n","                # context left\n","                left = ''\n","                for char in text[indx - width if not width > indx else 0: indx][::-1]:\n","                    if char == '\\n': # make '\\n' the boundary for context\n","                        break\n","                    left += char\n","                left = left[::-1].strip()\n","                # context right\n","                right = ''\n","                for char in text[indx + len(keyphrase): indx + len(keyphrase) + width]:\n","                  if char == '\\n': # make '\\n' the boundary for context\n","                    break\n","                  right += char\n","                right = right.strip()\n","                contexts.append((left, right))\n","        return contexts\n","get_lr_contexts('This is a test, lets see what test happens with the test', 'test', width=50, lower=False)"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('This is a', ', lets see what test happens with the test'),\n"," ('This is a test, lets see what', 'happens with the test'),\n"," ('is is a test, lets see what test happens with the', '')]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKE_ZrB-5Qxj","executionInfo":{"status":"ok","timestamp":1610997122777,"user_tz":-60,"elapsed":526,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"934b1dcf-503d-4e8d-9de9-0163c7b71f97"},"source":["from re import finditer\n","[item.span()[0] for item in finditer('test', 'This is a test, lets see what test happens with the test')]"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[10, 30, 52]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"wSZjc1g51so2"},"source":["## Experiments and snippets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"Gi_C2GrJ1ufk","executionInfo":{"status":"ok","timestamp":1610896819658,"user_tz":-60,"elapsed":3530,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"7adde0f6-308f-4b61-da7d-c41bc90a0f0a"},"source":["# revision = CRISPR_en.get_revision(revid=725648335)\n","# print(revision.url, \"\\n\")\n","# references = revision.get_references()\n","# reference = references[1]\n","# print(\"REFERENCE\\n\", reference.get_text())\n","# print(\"REFERENCE ID\\n\", reference.get_id())\n","# print(\"REFERENCE Number\\n\", reference.get_number_via_id())\n","# print(\"REFERENCE Superscript\\n\", reference.get_superscript(revision))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["https://en.wikipedia.org/w/index.php?title=CRISPR&oldid=725648335 \n","\n","REFERENCE\n"," Horvath P, Barrangou R (January 2010). \"CRISPR/Cas, the immune system of bacteria and archaea\". Science. 327 (5962): 167–70. Bibcode:2010Sci...327..167H. doi:10.1126/Science.1179555. PMID 20056882.\n","REFERENCE ID\n"," cite_note-pmid20056882-1\n","REFERENCE Number\n"," 1\n","REFERENCE Superscript\n"," [1]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'[1]'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"3NCx3QPvG2hf"},"source":["# def occurance_spans(keyphrase, article):\n","#   ''' \n","#   returns a list of time spans during which a keyphrase 'keyphrase' continously appears in an article 'a'\n","#   '''\n","#   gen = article.yield_revisions()\n","#   last_rev_was_match = False\n","#   spans = []\n","#   for revision in gen:\n","#     if keyphrase in revision.get_text():\n","#     # if keyphrase in i.get_text().split('\\nLocus structure\\n')[0]: # Trick 17 to exclude everything after history section and practially only search in histroy section\n","#       last_rev_that_matched = revision\n","#       if not last_rev_was_match:\n","#         span_beg = revision\n","#         last_rev_was_match = True\n","#     else:\n","#       if last_rev_was_match:\n","#         spans.append((span_beg, last_rev_that_matched))\n","#         last_rev_was_match = False\n","#   if last_rev_was_match:\n","#     spans.append((span_beg, last_rev_that_matched))\n","#   print(\n","#       '\"{}\" was found in the following revisions of {}:\\n{}'.format(keyphrase, 'Test', '\\n'.join(['Span {}: {} - {}'.format(indx + 1, revision[0].timestamp, revision[1].timestamp) for indx,revision in enumerate(spans)])) \n","#       if spans else '\"{}\" was not found in any revision of \"{}\"'.format(keyphrase, 'Test')\n","#       )\n","#   return spans"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BG3rjcwdsAvH"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"ZCghcj7J1tZb"},"source":["def cluster_lr_contexts(lr_contexts, score_cutoff=75, scorer=fuzz.ratio, sort_by=['left_id','timestamp']):\n","  # determine communities of similarity for left and right contexts\n","  df = pd.DataFrame(lr_contexts, columns=['revindx','timestamp','left_id','left','keyphrase','right','right_id','revid','url','user','userid','comment',])\n","  for side in ['left', 'right']:\n","    G = nx.Graph()\n","    unique_contexts = list(df[side].unique())\n","    length = len(unique_contexts)\n","    for query in unique_contexts:\n","      indx = unique_contexts.index(query) + 1\n","      if indx <= length:\n","        scores = process.extractBests(query, unique_contexts[unique_contexts.index(query) + 1:], score_cutoff=score_cutoff, limit=length, scorer=scorer) # https://github.com/seatgeek/fuzzywuzzy/blob/master/fuzzywuzzy/process.py\n","        edges = [(query, i[0]) for i in scores]\n","        G.add_edges_from(edges)\n","    # add community_ids for lonesome contexts as well\n","    unique_contexts = set(unique_contexts)\n","    community_dic = community_louvain.best_partition(G)\n","    if community_dic:\n","      community_contexts = set(community_dic.keys())\n","      lonesome_contexts = unique_contexts - community_contexts\n","      lonesome_dic = {k:v for v,k in enumerate(lonesome_contexts,start=max(community_dic.values())+1)}\n","      community_dic.update(lonesome_dic) # hiermit nun 'lid' und 'rid' Spalten befüllen...\n","    else:\n","      community_dic = {k:v for v,k in enumerate(unique_contexts)}\n","    # add community_ids to df\n","    column = df.apply(lambda row: community_dic[row[side]], axis=1)\n","    df['{}_id'.format(side)] = column\n","    df.sort_values(by=sort_by)\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYiX-CAS8F31"},"source":["## Extract keyphrase contexts"]},{"cell_type":"markdown","metadata":{"id":"A-dbVRM88bS_"},"source":["### Define keyphrases"]},{"cell_type":"code","metadata":{"id":"G3gVN7VqHAmw"},"source":["keyphrases = [                              \n","        # discoveries\n","        'first',\n","        'discover',\n","        # naming things\n","        'name', # CRISPR acronym\n","        'acronym', # CRISPR acronym\n","        # patents\n","        'patent',\n","        'property',\n","        'right',\n","        # clinical trials\n","        'trial',\n","        'clinical',\n","        'patient',\n","        'approve', # e.g. FDA approved...\n","        # awards\n","        'award',\n","        'prize',\n","        'nobel',\n","        # people:\n","        'doudna', \n","        'charpentier', \n","        'zhang', \n","        'šikšnys', # with all accents\n","          'šiksnys', # only first accent\n","          'sikšnys', # only second accent\n","          'siksnys', # no accents\n","        'gašiūnas', # with all accents\n","          'gasiūnas', # only first accent\n","          'gašiunas', # only second accent\n","          'gasiunas', # no accents\n","        'horvath', # auch im Zusammenhang mit Šikšnys, aber auch so interessant\n","        'barrangou', # auch im Zusammenhang mit Šikšnys, aber auch so interessant\n","        'church',\n","        'brouns',\n","        'mojica',\n","        'ishino',\n","        'jansen',\n","        'marraffini',\n","        'sontheimer',\n","        'koonin',\n","        'moineau',\n","        'bolotin',\n","        'sorokin',\n","        'makarova',\n","        'pourcel',\n","        'overbeek',\n","        'liang',\n","        ' xu', # added space!\n","        'jiankui',\n","        'nishimasu',\n","        'shibata',\n","        'lander', # review\n","        # institutions:\n","        'osaka',\n","        'alicante',\n","        'danisco',\n","        'broad',\n","        'harvard',\n","        'university of california', \n","        'berkeley', \n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sHS-YLj18n7_"},"source":["### Extract and cluster keyphrase contexts"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gD5w946_-hmg","outputId":"74235c4c-bb26-410a-b6b2-20a624dfa117"},"source":["articles = [    \n","  CRISPR_en,\n","  CRISPR_gene_editing_en\n","]\n","\n","for article in articles:\n","  print(article.name)\n","  for keyphrase in keyphrases:\n","    print('\\t', keyphrase)\n","    lr_contexts = [\n","      (revision.index, revision.timestamp.string, 0, left, keyphrase, right, 0, revision.revid, revision.url, revision.user, revision.userid, revision.comment,)\n","      for revision in article.yield_revisions()\n","      for left, right in revision.get_lr_contexts(keyphrase, width=100, lower=True)\n","      ]\n","    df = cluster_lr_contexts(lr_contexts, score_cutoff=75, scorer=fuzz.ratio, sort_by=['left_id','timestamp'])\n","    df.to_excel(path_extractions + '/keyphrase_contexts/{}_{}.xlsx'.format(keyphrase, article.name))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CRISPR_en\n","\t first\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4rG5Bsw18IoO"},"source":["## Extract citation contexts"]},{"cell_type":"markdown","metadata":{"id":"oUTw67zW8-3y"},"source":["### Define citations"]},{"cell_type":"code","metadata":{"id":"-Tf7IXZRG5HT"},"source":["citations = [\n","        {'DOI':'10.1016/j.cell.2014.05.010',    'PMID':'24906146',  'PMC':'',         'name':'Hsu_et_al_2014'},\n","        {'DOI':'10.1126/science.1258096',       'PMID':'25430774',  'PMC':'',         'name':'Doudna_Charpentier_2014'},\n","        {'DOI':'10.1038/s41467-018-04252-2',    'PMID':'29765029',  'PMC':'5953931',  'name':'Adli_2018'},\n","        {'DOI':'10.1038/522020a',               'PMID':'26040877',  'PMC':'',         'name':'Ledford_2015'},\n","        {'DOI':'10.1126/science.341.6148.833',  'PMID':'23970676',  'PMC':'',         'name':'Pennisi_2013'},\n","        {'DOI':'10.1016/j.cell.2015.12.041',    'PMID':'26771483',  'PMC':'',         'name':'Lander_2016'},\n","        {'DOI':'10.1016/j.tim.2016.06.005',     'PMID':'27401123',  'PMC':'',         'name':'Mojica_Montoliu_2016'},\n","        {'DOI':'10.1111/febs.13766',            'PMID':'27234458',  'PMC':'',         'name':'Mojica_Rodriguez-Valera_2016'},\n","        {'DOI':'10.1128/JB.00580-17',           'PMID':'29358495',  'PMC':'5847661',  'name':'Ishino_et_al_2018'},\n","        {'DOI':'10.1016/j.coviro.2015.03.011',  'PMID':'25914022',  'PMC':'',         'name':'van_Erp_et_al_2015'},\n","        {'DOI':'10.1007/s12038-015-9532-6',     'PMID':'25740136',  'PMC':'',         'name':'Morange_2015a'},\n","        {'DOI':'10.1038/nbt.3160',              'PMID':'25748913',  'PMC':'',         'name':'Sherkov_2015'},\n","        {'DOI':'10.1007/978-3-642-34657-6_1',   'PMID':'',          'PMC':'',         'name':'Mojica_Garett_2013'},\n","        {'DOI':'10.1038/535342a',               'PMID':'27443723',  'PMC':'',         'name':'Ledford_2016a'},\n","        {'DOI':'10.1016/bs.pmbts.2017.10.001',  'PMID':'29150001',  'PMC':'',         'name':'Han_She_2017'},\n","        {'DOI':'10.1007/s12038-015-9575-8',     'PMID':'26648028',  'PMC':'',         'name':'Morange_2015b'},\n","        {'DOI':'10.1002/jgm.2963',              'PMID':'28623876',  'PMC':'',         'name':'Liang_et_al_2017'},\n","        {'DOI':'10.1007/s00284-018-1547-4',     'PMID':'30078067',  'PMC':'',         'name':'Javed_et_al_2018'},\n","        {'DOI':'10.1038/537460a',               'PMID':'27652544',  'PMC':'',         'name':'Ledford_2016b'},\n","        {'DOI':'10.1007/978-3-642-34657-6_3',   'PMID':'',          'PMC':'',         'name':'Makarova_Koonin_2013'},\n","        {'DOI':'10.1007/978-3-642-34657-6_11',  'PMID':'',          'PMC':'',         'name':'Horvath_et_al_2013'},\n","        {'DOI':'10.1038/d41586-020-02765-9',    'PMID':'33028993',  'PMC':'',         'name':'Ledford_Callaway_2020'}, \n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFkaVr_I9AwQ"},"source":["### Extract and cluster citation contexts"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvpWEDWs9kmH","executionInfo":{"status":"ok","timestamp":1610905359233,"user_tz":-60,"elapsed":4129218,"user":{"displayName":"Arno Simons","photoUrl":"","userId":"02448447039943627700"}},"outputId":"df3b838c-1e9b-4e00-890d-a52d161eefb4"},"source":["articles = [    \n","  CRISPR_en,\n","  # CRISPR_gene_editing_en\n","]\n","\n","for article in articles:\n","  print(article.name)\n","  for citation in citations:\n","    print('\\t', citation['name'])\n","    lr_contexts = [\n","      (revision.index, revision.timestamp.string, 0, left, target_match.get_superscript(revision), right, 0, revision.revid, revision.url, revision.user, revision.userid, revision.comment,)\n","      for revision in article.yield_revisions()\n","      for id_type, id_num in {k:v for k,v in citation.items() if v and any(k==i for i in ['DOI', 'PMC', 'PMID'])}.items()\n","      for target_match in set(\n","          reference for reference in revision.get_references() if reference.get_identifiers()[id_type] == id_num\n","      )\n","      for left, right in revision.get_lr_contexts(target_match.get_superscript(revision), width=100, lower=True)\n","      ]\n","    df = cluster_lr_contexts(lr_contexts, score_cutoff=75, scorer=fuzz.ratio, sort_by=['left_id','timestamp'])\n","    df.to_excel(path_extractions + '/citation_contexts/{}_{}.xlsx'.format( citation['name'], article.name))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CRISPR_en\n","\t Doudna_Charpentier_2014\n","\t Adli_2018\n","\t Pennisi_2013\n","\t Lander_2016\n","\t Mojica_Montoliu_2016\n","\t Mojica_Rodriguez-Valera_2016\n","\t Ishino_et_al_2018\n","\t van_Erp_et_al_2015\n","\t Morange_2015a\n","\t Sherkov_2015\n","\t Mojica_Garett_2013\n","\t Ledford_2016a\n","\t Han_She_2017\n","\t Morange_2015b\n","\t Liang_et_al_2017\n","\t Javed_et_al_2018\n","\t Ledford_2016b\n","\t Makarova_Koonin_2013\n","\t Horvath_et_al_2013\n","\t Ledford_Callaway_2020\n"],"name":"stdout"}]}]}